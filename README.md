# ğŸ† Fine-Tuning T5 for Question Answering

This repository contains the implementation for **fine-tuning T5** on a **Question Answering (QA)** dataset. The model leverages **Hugging Face's Transformers** library, utilizing **pre-trained T5 models** for better accuracy in answering questions.

## ğŸ“‚ Project Structure

```
ğŸ“¦ Fine-Tuning-T5-QA
â”‚â”€â”€ ğŸ“œ config.json          # Model configuration file
â”‚â”€â”€ ğŸ“œ generation_config.json  # Text generation settings
â”‚â”€â”€ ğŸ“œ special_tokens_map.json  # Special tokens configuration
â”‚â”€â”€ ğŸ“œ spiece.model         # SentencePiece tokenizer model
â”‚â”€â”€ ğŸ“œ tokenizer.json       # Tokenizer vocabulary and merges
â”‚â”€â”€ ğŸ“œ tokenizer_config.json  # Tokenizer settings
â”‚â”€â”€ ğŸ“œ T5_QA.ipynb          # Jupyter Notebook for fine-tuning and inference
```

## ğŸ“Œ Features

âœ… Fine-tunes **T5** for **extractive & abstractive QA**  
âœ… Implements **custom tokenization** using SentencePiece  
âœ… Leverages **Hugging Face Transformers** for training & inference  
âœ… Supports **structured model checkpoints**  
âœ… Optimized for **GPU acceleration** using PyTorch & Safetensors  

## ğŸ›  Installation

Ensure you have Python 3.8+ installed and install the required dependencies:

```bash
pip install torch transformers sentencepiece safetensors
```

## ğŸš€ Fine-Tuning T5 for QA

You can fine-tune the model using the provided Jupyter Notebook:

```bash
jupyter notebook T5_QA.ipynb
```

Alternatively, execute the following script to load and train the model:

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load tokenizer and model
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Example input
input_text = "question: What is AI? context: Artificial Intelligence (AI) is the simulation of human intelligence in machines."
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# Generate an answer
output_ids = model.generate(input_ids, max_length=50)
answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Answer:", answer)
```

## ğŸ“Š Generating Answers with Fine-Tuned Model

Once the model is trained, you can use it to answer questions:

```python
from transformers import pipeline

qa_pipeline = pipeline("text2text-generation", model="./model.safetensors", tokenizer=tokenizer)
output = qa_pipeline("question: What is machine learning? context: Machine learning is a subset of AI that focuses on learning from data.")
print(output)
```


---

ğŸ“§ **Contact:** ali.abdien.omar@gmail.com  

---
